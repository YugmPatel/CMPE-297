# ðŸš€ Modern AI with Unsloth.ai

This repository contains a complete, multi-notebook series demonstrating **modern fine-tuning and reinforcement learning techniques** using **Unsloth.ai** across diverse use cases.
Each use case includes: a runnable Colab notebook, dataset notes, clear inputs/outputs, and a **YouTube video walkthrough** showing the code tour, training run, and inference demo.

---

## ðŸŽ¯ Goals

1. Show end-to-end workflows in Unsloth.ai for:
   â€¢ Full fine-tuning of small LLMs
   â€¢ Parameter-efficient fine-tuning (LoRA)
   â€¢ Preference-based Reinforcement Learning (RL) with chosen datasets
   â€¢ GRPO-style reasoning RL with model-generated answers
   â€¢ Continued pretraining to extend model knowledge (e.g., new language)

2. Provide reproducible notebooks and structured artifacts suitable for grading and future reuse.

---

## ðŸ“š References & Hints

â€¢ Unsloth Notebooks Hub (Kaggle/Colab):
[https://github.com/unslothai/notebooks](https://github.com/unslothai/notebooks)

â€¢ Unsloth Finetuning Guide:
[https://docs.unsloth.ai/get-started/fine-tuning-llms-guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)

â€¢ Reinforcement Learning Guide:
[https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide)

â€¢ GRPO Reasoning Tutorial:
[https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo)

â€¢ Unsloth R1 Reasoning Blog:
[https://unsloth.ai/blog/r1-reasoning](https://unsloth.ai/blog/r1-reasoning)

â€¢ Continued Pretraining:
[https://docs.unsloth.ai/basics/continued-pretraining](https://docs.unsloth.ai/basics/continued-pretraining)

â€¢ Export to Ollama:
[https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama)

â€¢ Example Kaggle notebook:
[https://www.kaggle.com/code/kingabzpro/fine-tuning-llms-using-unsloth](https://www.kaggle.com/code/kingabzpro/fine-tuning-llms-using-unsloth)

â€¢ LoRA with Ollama (templates):
[https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0](https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0)

---

## ðŸ§ª Use Cases (Colabs)

a) Colab 1 â€” Full Finetuning (Small Model)
â€¢ Model: smolLM2 135M (or unsloth/gemma-3-1b-it-unsloth-bnb-4bit) with full_finetuning enabled
â€¢ Task: choose coding or chat; demonstrate compatible chat templates
â€¢ Deliverables: training run, evaluation samples, inference examples, YouTube walkthrough

b) Colab 2 â€” LoRA Parameter-Efficient Finetuning
â€¢ Same dataset and task as Colab 1, but convert to LoRA
â€¢ Compare runtime, GPU memory, and output quality vs. full FT
â€¢ Deliverables: hyperparameter notes, side-by-side sample outputs, YouTube walkthrough

c) Colab 3 â€” Reinforcement Learning with Preferences
â€¢ Dataset contains input, preferred output, rejected output (pairwise or listwise)
â€¢ Show reward signal design and training stability heuristics
â€¢ Deliverables: training curves, preference accuracy snapshots, qualitative comparisons, YouTube walkthrough

d) Colab 4 â€” GRPO Reasoning RL
â€¢ Use problem-only datasets; model generates answers; optimize via GRPO
â€¢ Highlight reasoning traces, token budget, and stop sequences
â€¢ Deliverables: sample problems, rationales, reward trajectory, YouTube walkthrough

e) Colab 5 â€” Continued Pretraining (New Language)
â€¢ Extend base modelâ€™s vocabulary/knowledge on a curated corpus for a new language or domain
â€¢ Show tokenizer/encoding considerations and eval prompts pre/post CPT
â€¢ Deliverables: corpus sourcing and cleaning notes, perplexity or proxy metrics, qualitative outputs, YouTube walkthrough

Optional Enhancements (documented in extras/):
â€¢ Continue fine-tuning from a custom checkpoint
â€¢ Mental-health chatbot case study with Phi-3 (ethical guardrails, safe-completion prompts)
â€¢ Export finetuned model to Ollama and demonstrate local inference

---

## ðŸ§° Models You May Reference (Open Weights)

Choose at least the small model for Colab 1 and reuse across others when appropriate:

â€¢ smolLM2 135M

â€¢ unsloth/gemma-3-1b-it-unsloth-bnb-4bit

â€¢ Llama 3.1 (8B), Llama 3 (8B)

â€¢ Mistral v0.3 (7B), Mistral NeMo (12B)

â€¢ Gemma 2 (2B, 9B)

â€¢ Phi-3 / Phi-3.5 (mini/medium)

â€¢ Qwen2 (7B)

â€¢ TinyLlama

**Note:** For grading and reproducibility, prioritize smaller models for full FT; use larger models for LoRA or RL when feasible.

---

## ðŸ§¾ Datasets & Formatting

â€¢ Clearly document the dataset source and license in each folderâ€™s dataset_notes.md.

â€¢ For chat/coding tasks, include the prompt/response schema and any chat template used.

â€¢ For RL with preferences, store each sample with input, preferred, rejected output, and any metadata (source, difficulty).

â€¢ For GRPO, list problem statements, acceptance criteria, and reward function.

â€¢ For continued pretraining, describe corpus cleaning, deduplication, and tokenization notes.

---

## âœ… Deliverables Checklist

For each Colab folder:

â€¢ A Colab notebook that completes successfully end-to-end

â€¢ Short README (dataset_notes.md or corpus_prep.md) describing inputs, schema, and evaluation

â€¢ At least three qualitative examples of before/after behavior

â€¢ Training artifacts: logs, charts, or screenshots in results/

â€¢ A YouTube video walkthrough link in video/ with:
â€“ Problem statement and dataset explanation
â€“ Code tour line-by-line for important cells
â€“ Live training run or loaded logs with commentary
â€“ Inference demos showing inputs and outputs
â€“ Summary of lessons learned and limitations

---

## ðŸ”¬ Evaluation & Reporting

**Quality**
â€¢ Human-readable examples with success/failure notes
â€¢ Optional automatic metrics (e.g., exact match on reasoning tasks, BLEU/ROUGE for text, preference accuracy for RL)

**Efficiency**
â€¢ GPU type and runtime
â€¢ Batch sizes, sequence lengths, LoRA ranks
â€¢ Memory observations and stability tips

**Comparisons**
â€¢ Full FT vs. LoRA: cost, speed, output quality
â€¢ SFT vs. RL vs. GRPO outcomes on the same prompts

---

## ðŸ›  Environment & Dependencies

â€¢ Google Colab (T4/A100/L4 where available)

â€¢ Unsloth.ai libraries and their documented dependencies

â€¢ Optional: Kaggle integration for datasets or running Kaggle notebooks

â€¢ Note any special accelerator/runtime selection in the top of each notebook

---

## ðŸ”„ Reproducibility Notes

â€¢ Fix random seeds where possible; log package versions

â€¢ Save and reference checkpoints and tokenizer versions

â€¢ Document any cloud storage used for artifacts

â€¢ When exporting to Ollama, record the exact export command, model name, and inference settings in extras/export_to_ollama_notes.md

---

## ðŸ”’ Ethics & Safety

â€¢ If you build sensitive assistants (e.g., mental-health), include clear disclaimers and safe-completion prompts

â€¢ Filter or anonymize personal data in training corpora

â€¢ Respect dataset licenses and cite all sources
